{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:44.575987Z",
     "start_time": "2019-03-09T15:42:38.240589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dagob\\Anaconda3\\lib\\site-packages\\dask\\config.py:168: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  data = yaml.load(f.read()) or {}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Client</h3>\n",
       "<ul>\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:7017\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3>Cluster</h3>\n",
       "<ul>\n",
       "  <li><b>Workers: </b>4</li>\n",
       "  <li><b>Cores: </b>4</li>\n",
       "  <li><b>Memory: </b>8.51 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: scheduler='tcp://127.0.0.1:7017' processes=4 cores=4>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a really small blocksize to force dask to work with multiple partitions. Else it would be the same as just working with pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:44.738319Z",
     "start_time": "2019-03-09T15:42:44.575987Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p = pd.read_csv(\"../data/hour.csv\")\n",
    "df_d = dd.read_csv(\"https://s3.eu-central-1.amazonaws.com/ie-mbd-advpython-ml-bikesharing-dask/hour.csv\",\n",
    "                   blocksize=\"250KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.memory_usage().sum().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:44.775859Z",
     "start_time": "2019-03-09T15:42:44.738319Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:44.838348Z",
     "start_time": "2019-03-09T15:42:44.775859Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.1. Identifying null values\n",
    "##### According to the below the code this dataset does not have any null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:44.907405Z",
     "start_time": "2019-03-09T15:42:44.838348Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the \"query\" to calculate something but it's lazy evaluated until we call compute into it. IF we're working on an actualy cluster, then we can call client.persist(df_d) so that each node will do its calculation and then we can gather the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = df_d.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nulls = client.persist(nulls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "then we can compute or gather-compute the results back to our machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "client.gather(nulls).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Daks-persist-gather-compute Example**\n",
    "This is an example to showcase how using persist can offload some computation in intermediate steps so that the final calculation is faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_atemp = df_d.groupby('mnth')['atemp'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_atemp = client.persist(mean_atemp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ This is where the calculation will happen so we can have intermediate results to make subsequent queries faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.gather(mean_atemp).compute()  # This is almost instant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### columns' name modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:45.656355Z",
     "start_time": "2019-03-09T15:42:45.640736Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p.rename(\n",
    "    columns={\n",
    "        \"dteday\": \"datetime\",\n",
    "        \"weathersit\": \"weather_condition\",\n",
    "        \"cnt\": \"total_bike_rented\",\n",
    "    },\n",
    "    inplace=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = df_d.rename(columns={\n",
    "    \"dteday\": \"datetime\",\n",
    "        \"weathersit\": \"weather_condition\",\n",
    "        \"cnt\": \"total_bike_rented\",\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns' type modification:\n",
    "##### Taking a look at the dataset information, we see that some columns are not in the appropriate data type. For instance, datetime is an object while it should be in date format. also we have some columns that re factor but in the dataset are presented as integer, season, year, moth, hr, holiday, workingday, weather_condition are of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:45.756670Z",
     "start_time": "2019-03-09T15:42:45.725422Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p[\"datetime\"] = pd.to_datetime(df_p.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d[\"datetime\"] = dd.to_datetime(df_d.datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:45.778829Z",
     "start_time": "2019-03-09T15:42:45.756670Z"
    }
   },
   "outputs": [],
   "source": [
    "features_to_transform = [\"season\",\"yr\",\"mnth\",\"hr\",\"holiday\",\"weekday\",\"workingday\",\"weather_condition\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:45.810087Z",
     "start_time": "2019-03-09T15:42:45.778829Z"
    }
   },
   "outputs": [],
   "source": [
    "def type_shifter(df, features, new_type):\n",
    "    \"\"\"this function takes the selected features of a data frame and \n",
    "    cast them to the new_type\"\"\"\n",
    "    for i in features:\n",
    "        df[i] = df[i].astype(new_type)\n",
    "    print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:45.894798Z",
     "start_time": "2019-03-09T15:42:45.810087Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p = type_shifter(df_p, features_to_transform, \"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d = type_shifter(df_d, features_to_transform, \"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Renaming the categorical variables' levels:\n",
    "\n",
    "#### In the priginal dataset all the weekdays and months are presented as integer, although we have already casted them to categorical type, in order to make them more informative, we would change the levels' names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:45.910423Z",
     "start_time": "2019-03-09T15:42:45.894798Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p['weekday']=df_p['weekday'].map({\n",
    "        0: \"Sunday\",\n",
    "        1: \"Monday\",\n",
    "        2: \"Tuesday\",\n",
    "        3: \"Wednesday\",\n",
    "        4: \"Thursday\",\n",
    "        5: \"Friday\",\n",
    "        6: \"Saturday\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d['weekday']=df_d['weekday'].map({\n",
    "        0: \"Sunday\",\n",
    "        1: \"Monday\",\n",
    "        2: \"Tuesday\",\n",
    "        3: \"Wednesday\",\n",
    "        4: \"Thursday\",\n",
    "        5: \"Friday\",\n",
    "        6: \"Saturday\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here we will check which season is which numbner based on the unique months based on that season**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we don't want to recompute everythin every time, especially if we're dealing with huge ammounts of data, we will convert a small sample on the dataset to a pandas dataframe (that would fit in memory) and then check things as we don't need all the data to determine the seasons by looking at the unique months"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Seasons from the unique months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p.groupby('season')['mnth'].apply(np.unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that season 1 is winter and then Spring, Summer, Autumn come"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df_d.sample(frac=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_df[sample_df['season'] == 1]['mnth'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df['season'] == 2]['mnth'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df['season'] == 3]['mnth'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df[sample_df['season'] == 4]['mnth'].unique().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.111064Z",
     "start_time": "2019-03-09T15:42:46.079817Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p['season']=df_p['season'].map({1: \"Winter\", 2: \"Spring\", 3: \"Summer\", 4: \"Autumn\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d['season'] = df_d['season'].map({1: \"Winter\", 2: \"Spring\", 3: \"Summer\", 4: \"Autumn\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.142325Z",
     "start_time": "2019-03-09T15:42:46.111064Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p['mnth']=df_p['mnth'].map({\n",
    "        1: \"01-Jan\",\n",
    "        2: \"02-Feb\",\n",
    "        3: \"03-Mar\",\n",
    "        4: \"04-Apr\",\n",
    "        5: \"05-May\",\n",
    "        6: \"06-Jun\",\n",
    "        7: \"07-Jul\",\n",
    "        8: \"08-Aug\",\n",
    "        9: \"09-Sep\",\n",
    "        10: \"10-Oct\",\n",
    "        11: \"11-Nov\",\n",
    "        12: \"12-Dec\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d['mnth']=df_d['mnth'].map({\n",
    "        1: \"01-Jan\",\n",
    "        2: \"02-Feb\",\n",
    "        3: \"03-Mar\",\n",
    "        4: \"04-Apr\",\n",
    "        5: \"05-May\",\n",
    "        6: \"06-Jun\",\n",
    "        7: \"07-Jul\",\n",
    "        8: \"08-Aug\",\n",
    "        9: \"09-Sep\",\n",
    "        10: \"10-Oct\",\n",
    "        11: \"11-Nov\",\n",
    "        12: \"12-Dec\",\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.180121Z",
     "start_time": "2019-03-09T15:42:46.142325Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p['yr']=df_p['yr'].map({0:'2011',1:'2012'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d['yr']=df_d['yr'].map({0:'2011',1:'2012'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.211367Z",
     "start_time": "2019-03-09T15:42:46.180121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p['weather_condition']=df_p['weather_condition'].map({1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.211367Z",
     "start_time": "2019-03-09T15:42:46.180121Z"
    }
   },
   "outputs": [],
   "source": [
    "df_d['weather_condition']=df_d['weather_condition'].map({1: \"A\", 2: \"B\", 3: \"C\", 4: \"D\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.280420Z",
     "start_time": "2019-03-09T15:42:46.211367Z"
    }
   },
   "outputs": [],
   "source": [
    "df_p.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_d.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "#### In order to see if there is any specific trend in different time intervals we present the target variable over month, day and hour to get a more clear vision of the target variable evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: For distributed datasets we would normally sample the data to visualize as it would require all the data to come into a single machine, which if we were using enough data to warrant distribution then it would probably crash the machine. So for the sake of this example, we will sample the dataset every time we need to compute to simulate how this would work with a bigger dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of records\n",
    "df_p.count()['instant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count of records computing everything\n",
    "df_d['instant'].count().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count by Partition (This is stupid as it can't be parallelized)\n",
    "for i in range(df_d.npartitions):\n",
    "    count = df_d['instant'].get_partition(i).count().compute()\n",
    "    print(\"Partition {}: {} records\".format(i, count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will sample and compute before visualizing with 30% of the dataframe as an example. Normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:46.296054Z",
     "start_time": "2019-03-09T15:42:46.280420Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.set_context('talk')\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (30, 10),\n",
    "          'axes.labelsize': 'large',\n",
    "          'axes.titlesize':'large',\n",
    "          'xtick.labelsize':'large',\n",
    "          'ytick.labelsize':'large'}\n",
    "\n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:48.787466Z",
     "start_time": "2019-03-09T15:42:46.296054Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=df_p[[\"hr\", \"total_bike_rented\", \"season\"]],\n",
    "    x=\"hr\",\n",
    "    y=\"total_bike_rented\",\n",
    "    hue=\"season\",\n",
    "    ax=ax,\n",
    "    err_style=None\n",
    ")\n",
    "ax.set(title=\"Hourly distribution of counts in different seasons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=df_d[[\"hr\", \"total_bike_rented\", \"season\"]].sample(frac=0.3).compute(),\n",
    "    x=\"hr\",\n",
    "    y=\"total_bike_rented\",\n",
    "    hue=\"season\",\n",
    "    ax=ax,\n",
    "    err_style=None\n",
    ")\n",
    "ax.set(title=\"Hourly distribution of counts in different seasons\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The above graph shows that bike rental business is fairly seasonal and aslo correlated with the day time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:52.321640Z",
     "start_time": "2019-03-09T15:42:48.787466Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=df_p[[\"hr\", \"total_bike_rented\", \"weekday\"]],\n",
    "    x=\"hr\",\n",
    "    y=\"total_bike_rented\",\n",
    "    hue=\"weekday\",\n",
    "    ax=ax,\n",
    "    err_style=None\n",
    "    \n",
    "    \n",
    ")\n",
    "ax.set(title=\" Hourly distribution of counts over the weekdays\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:52.321640Z",
     "start_time": "2019-03-09T15:42:48.787466Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=df_d[[\"hr\", \"total_bike_rented\", \"weekday\"]].sample(frac=0.3).compute(),\n",
    "    x=\"hr\",\n",
    "    y=\"total_bike_rented\",\n",
    "    hue=\"weekday\",\n",
    "    ax=ax,\n",
    "    err_style=None\n",
    "    \n",
    "    \n",
    ")\n",
    "ax.set(title=\" Hourly distribution of counts over the weekdays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Above graph shows us that the behaviour is fundamentally different between weekdays and weekends where on one the pattern seems to revolve around \"commuting\" and over the weekds noon-earlyafternoon leisure\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:53.278069Z",
     "start_time": "2019-03-09T15:42:52.321640Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=df_p[[\"mnth\", \"total_bike_rented\", \"yr\"]], x=\"mnth\", y=\"total_bike_rented\", ax=ax,\n",
    ")\n",
    "ax.set(title=\"Monthly distribution of counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:53.278069Z",
     "start_time": "2019-03-09T15:42:52.321640Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(\n",
    "    data=df_d[[\"mnth\", \"total_bike_rented\", \"yr\"]].sample(frac=0.3).compute(), x=\"mnth\", y=\"total_bike_rented\", ax=ax,\n",
    ")\n",
    "ax.set(title=\"Monthly distribution of counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can see that the usage increases during the summer months and declines in the winter months which is logical **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:53.710715Z",
     "start_time": "2019-03-09T15:42:53.278069Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "sns.boxplot(data=df_p[['total_bike_rented',\n",
    "                          'casual',\n",
    "                          'registered']],ax=ax,width=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:53.710715Z",
     "start_time": "2019-03-09T15:42:53.278069Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "sns.boxplot(data=df_d[['total_bike_rented',\n",
    "                          'casual',\n",
    "                          'registered']].sample(frac=0.3).compute()\n",
    "            ,ax=ax,width=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features correlation\n",
    "\n",
    "#### In order to avoid futrther error in the modeling part it is better to remove potential multicollinearity between features; therefore, we should measure the correlation coefficient between different featuers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:54.328396Z",
     "start_time": "2019-03-09T15:42:53.710715Z"
    }
   },
   "outputs": [],
   "source": [
    "correlation=df_p[['temp','atemp','hum','windspeed','casual','registered','total_bike_rented']].corr()\n",
    "#correlation.style.background_gradient(cmap='GnBu').set_precision(2)\n",
    "sns.heatmap(correlation,cmap='coolwarm',square=True,center=0,annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation=df_d[['temp','atemp','hum','windspeed','casual','registered','total_bike_rented']].sample(frac=0.3).corr().compute()\n",
    "#correlation.style.background_gradient(cmap='GnBu').set_precision(2)\n",
    "sns.heatmap(correlation,cmap='coolwarm',square=True,center=0,annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### As could be seen above atemp and temp are highly correlated so we will only keep temp and drop the atem from the dataset and other side we cannot say that anu other feature is absolutely useless for Predicting the target Variable so we will keep the other numeric features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:54.350591Z",
     "start_time": "2019-03-09T15:42:54.328396Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(\"atemp\",1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As the goal of this exercise is to predict the total_bike_rented we will remove casual and registered from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:54.381838Z",
     "start_time": "2019-03-09T15:42:54.350591Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop([\"casual\",\"registered\"],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:54.450900Z",
     "start_time": "2019-03-09T15:42:54.381838Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:54.466504Z",
     "start_time": "2019-03-09T15:42:54.450900Z"
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring numeric features:\n",
    "\n",
    "Linear regression algorithm is highly sensitive to the numerical features distribution and also outliers. It means if numeric features are highly skewd probably the regression model would be affected in a negative way. Outliers also are quiet important, and having some outliers in a linear model could change the result totally; therefore in this part we focus on cleaning the numeric values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:54.513393Z",
     "start_time": "2019-03-09T15:42:54.466504Z"
    }
   },
   "outputs": [],
   "source": [
    "numeric_cols= list(df.select_dtypes(include=np.number).columns.values)\n",
    "numeric_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:55.384067Z",
     "start_time": "2019-03-09T15:42:54.513393Z"
    }
   },
   "outputs": [],
   "source": [
    "counter=1\n",
    "for i in df[numeric_cols].columns.values:\n",
    "    plt.subplot(1, len(numeric_cols),counter)\n",
    "    plt.hist(df[i])\n",
    "    plt.title(str(i)+\" \"+ \"distribution\")\n",
    "    counter+=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Considering the sensiticity of the Linear regression model to numeric features' skewness we try modifiy this by transforming variables with skewness higher than our threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:55.399690Z",
     "start_time": "2019-03-09T15:42:55.384067Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import skew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:55.453175Z",
     "start_time": "2019-03-09T15:42:55.399690Z"
    }
   },
   "outputs": [],
   "source": [
    "df[['windspeed','hum','temp','total_bike_rented']].apply(lambda x: abs(skew(x))>0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### As could be seen above our target variable is highly skewed and needs to get transformed. Doing so, we take sqaure root of the target variable and the skewness would be in the acceptabel range now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:55.468786Z",
     "start_time": "2019-03-09T15:42:55.453175Z"
    }
   },
   "outputs": [],
   "source": [
    "skew(np.log(df.total_bike_rented))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As log transformation in this case cannot remove the skewness we try with sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:55.515664Z",
     "start_time": "2019-03-09T15:42:55.468786Z"
    }
   },
   "outputs": [],
   "source": [
    "skew(np.sqrt(df.total_bike_rented))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:55.584707Z",
     "start_time": "2019-03-09T15:42:55.515664Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['total_bike_rented']=np.sqrt(df.total_bike_rented)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:56.015539Z",
     "start_time": "2019-03-09T15:42:55.584707Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "sns.boxplot(data=df[['hum',\n",
    "                          'temp',\n",
    "                          'windspeed']],ax=ax,width=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring the boxplots for univariate analysis of outliers it seems that the numeric features containing outliers are hum and windspeed, however it is not enough yet to consider those points outliers. Therefore we do a multivariate analysis of windspeed and target variable to see if we can get more information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:56.633679Z",
     "start_time": "2019-03-09T15:42:56.015539Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(df['hum'],df['total_bike_rented'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:56.671527Z",
     "start_time": "2019-03-09T15:42:56.640197Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['hum']==0,'hum']=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:57.235174Z",
     "start_time": "2019-03-09T15:42:56.671527Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(df['windspeed'],df['total_bike_rented'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above scatter plot shows that probably winspeed values over 0.7 could be consider outliers, therefore our strategy at this point is to clip the windspeed values over 0.7 to o.7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:57.257373Z",
     "start_time": "2019-03-09T15:42:57.235174Z"
    }
   },
   "outputs": [],
   "source": [
    "df.loc[df['windspeed']>0.7,'windspeed']=0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:57.859148Z",
     "start_time": "2019-03-09T15:42:57.257373Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(df['windspeed'],df['total_bike_rented'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting categoriacal features to dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:57.906015Z",
     "start_time": "2019-03-09T15:42:57.859148Z"
    }
   },
   "outputs": [],
   "source": [
    "df=pd.get_dummies(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:57.959406Z",
     "start_time": "2019-03-09T15:42:57.906015Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we are going to use the last quarter of 2012 as the test set we will keep the index from which this quarter starts and it is 15212."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having finished the feature engineering part, we move to creating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:57.975034Z",
     "start_time": "2019-03-09T15:42:57.959406Z"
    }
   },
   "outputs": [],
   "source": [
    "y = df['total_bike_rented']\n",
    "X = df.drop(['datetime','total_bike_rented'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:42:58.006339Z",
     "start_time": "2019-03-09T15:42:57.975034Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train= X[:15211]\n",
    "X_test = X[15211:]\n",
    "y_train= y[:15211]\n",
    "y_test = y[15211:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:00.582714Z",
     "start_time": "2019-03-09T15:42:58.006339Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:01.100009Z",
     "start_time": "2019-03-09T15:43:00.582714Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with Linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:01.384746Z",
     "start_time": "2019-03-09T15:43:01.100009Z"
    }
   },
   "outputs": [],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "cross_validation = cross_val_score(lr,X_train,y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:01.400372Z",
     "start_time": "2019-03-09T15:43:01.384746Z"
    }
   },
   "outputs": [],
   "source": [
    "cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:01.600978Z",
     "start_time": "2019-03-09T15:43:01.400372Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_ridge=Ridge()\n",
    "lr_ridge.fit(X_train,y_train)\n",
    "Ridge_cv=cross_val_score(lr_ridge,X_train,y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:01.616610Z",
     "start_time": "2019-03-09T15:43:01.600978Z"
    }
   },
   "outputs": [],
   "source": [
    "Ridge_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:04.741745Z",
     "start_time": "2019-03-09T15:43:01.616610Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_lasso=Lasso(alpha=0.001)\n",
    "lr_lasso.fit(X_train,y_train)\n",
    "Lasso_cv=cross_val_score(lr_lasso,X_train,y_train,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:04.757370Z",
     "start_time": "2019-03-09T15:43:04.741745Z"
    }
   },
   "outputs": [],
   "source": [
    "Lasso_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:04.779547Z",
     "start_time": "2019-03-09T15:43:04.757370Z"
    }
   },
   "outputs": [],
   "source": [
    "lr_lasso.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:08.231105Z",
     "start_time": "2019-03-09T15:43:04.779547Z"
    }
   },
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.001,0.003,9)\n",
    "score = []\n",
    "for alpha in alphas:\n",
    "    model = Lasso(alpha=alpha, normalize=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    score.append(model.score(X_test, y_test))\n",
    "plt.plot(alphas, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:08.246756Z",
     "start_time": "2019-03-09T15:43:08.231105Z"
    }
   },
   "outputs": [],
   "source": [
    "best_lasso = Lasso(alpha=0.00150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:08.694928Z",
     "start_time": "2019-03-09T15:43:08.246756Z"
    }
   },
   "outputs": [],
   "source": [
    "best_lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting the test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:08.726141Z",
     "start_time": "2019-03-09T15:43:08.694928Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred=best_lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:09.110896Z",
     "start_time": "2019-03-09T15:43:08.726141Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.scatter(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:09.126514Z",
     "start_time": "2019-03-09T15:43:09.110896Z"
    }
   },
   "outputs": [],
   "source": [
    "result = pd.DataFrame({'truth':y_test, 'pred':y_pred})\n",
    "result['abs_diff'] = np.abs(y_test-y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:09.812629Z",
     "start_time": "2019-03-09T15:43:09.126514Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(data=result, x=\"pred\", y='truth', hue='abs_diff', palette='inferno')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For getting the true prediction we should undo the sqrt transformation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:09.828261Z",
     "start_time": "2019-03-09T15:43:09.812629Z"
    }
   },
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((y_test ** 2 - y_pred ** 2) ** 2))\n",
    "MSE = RMSE ** 2\n",
    "print(\"MSE::{}\".format(MSE))\n",
    "print(\"RMSE::{}\".format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:09.850420Z",
     "start_time": "2019-03-09T15:43:09.828261Z"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(np.mean((np.log(y_pred**2+1)-np.log(y_test**2+1))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the R^2 score is not good enough in both Lasso and Ridge regression we will try other algorithm. RandomForest seems to be a good match for this dataset, as it does not have many features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling using random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:43:10.013286Z",
     "start_time": "2019-03-09T15:43:09.850420Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:23.899421Z",
     "start_time": "2019-03-09T15:43:10.013286Z"
    }
   },
   "outputs": [],
   "source": [
    "gsc = GridSearchCV(\n",
    "        estimator=RandomForestRegressor(),\n",
    "        param_grid={\"max_depth\": (5, 10, 15, 20), \"n_estimators\": (50,100,150, 200)},\n",
    "        cv=3,\n",
    "        scoring=\"neg_mean_squared_error\",\n",
    "        verbose=1,\n",
    "        n_jobs=3,\n",
    "    )\n",
    "\n",
    "grid_result = gsc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:23.930737Z",
     "start_time": "2019-03-09T15:45:23.899421Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Best Score: {}. with Params: {}\".format(grid_result.best_score_, grid_result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having done the grid search we will do the model for the best parameters obtained from the grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:36.501479Z",
     "start_time": "2019-03-09T15:45:23.930737Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = grid_result.best_estimator_\n",
    "rf.fit(X_train, y_train)\n",
    "rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:36.990489Z",
     "start_time": "2019-03-09T15:45:36.501479Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.106415Z",
     "start_time": "2019-03-09T15:45:36.990489Z"
    }
   },
   "outputs": [],
   "source": [
    "rf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.222411Z",
     "start_time": "2019-03-09T15:45:37.106415Z"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.253650Z",
     "start_time": "2019-03-09T15:45:37.222411Z"
    }
   },
   "outputs": [],
   "source": [
    "RMSE = np.sqrt(np.mean((y_test ** 2 - y_pred_rf ** 2) ** 2))\n",
    "MSE = RMSE ** 2\n",
    "print(\"MSE::{}\".format(MSE))\n",
    "print(\"RMSE::{}\".format(RMSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We got much better result from random forest regressor R^2=0.86 while with linear regression the best R^2 was =0.74\n",
    "### MSE also for random forest is about half of that for linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHOTGUN STRATEGY!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.291424Z",
     "start_time": "2019-03-09T15:45:37.253650Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.322671Z",
     "start_time": "2019-03-09T15:45:37.291424Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.353951Z",
     "start_time": "2019-03-09T15:45:37.322671Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ARDRegression, BayesianRidge, ElasticNet, Lasso, Ridge\n",
    "from sklearn.linear_model import HuberRegressor, PassiveAggressiveRegressor, SGDRegressor, TheilSenRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.391774Z",
     "start_time": "2019-03-09T15:45:37.353951Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:45:37.407362Z",
     "start_time": "2019-03-09T15:45:37.391774Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:46:35.111354Z",
     "start_time": "2019-03-09T15:46:35.095718Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:46:50.431684Z",
     "start_time": "2019-03-09T15:46:35.697093Z"
    }
   },
   "outputs": [],
   "source": [
    "model_names = ['LinearRegression', 'BayesianRidge', 'ElasticNet', 'Lasso', 'Ridge',\n",
    "              'HuberRegressor', 'PassiveAggressiveRegressor', 'SGDRegressor', 'TheilSenRegressor']\n",
    "models = [LinearRegression(), BayesianRidge(), ElasticNet(), Lasso(), Ridge(),\n",
    "         HuberRegressor(), PassiveAggressiveRegressor(), SGDRegressor(), TheilSenRegressor()]\n",
    "\n",
    "mse = []\n",
    "mae = []\n",
    "msle = []\n",
    "medae = []\n",
    "r2 = []\n",
    "trained_model= []\n",
    "time_train = []\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    msle.append(mean_squared_log_error(y_test, abs(model.predict(X_test))))\n",
    "    medae.append(median_absolute_error(y_test, model.predict(X_test)))\n",
    "    r2.append(model.score(X_test, y_test))\n",
    "    \n",
    "    stop = time.time()\n",
    "    tr_time = stop - start\n",
    "    time_train.append(tr_time)\n",
    "    trained_model.append(model)\n",
    "    print(\"Model Trained\")\n",
    "models = pd.DataFrame(data = {'model': model_names, 'time_train': time_train,\n",
    "                      'MAE': mae, 'MSE': mse, 'MSLE': msle, 'MEDAE': medae, 'R2': r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:46:50.995718Z",
     "start_time": "2019-03-09T15:46:50.980184Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models = pd.concat([total_models, models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:46:52.052106Z",
     "start_time": "2019-03-09T15:46:52.014395Z"
    }
   },
   "outputs": [],
   "source": [
    "models.sort_values('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:48:30.942147Z",
     "start_time": "2019-03-09T15:48:30.926568Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor, BaggingRegressor,ExtraTreesRegressor, GradientBoostingRegressor, RandomForestRegressor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:48:42.127108Z",
     "start_time": "2019-03-09T15:48:33.657343Z"
    }
   },
   "outputs": [],
   "source": [
    "model_names = ['AdaBoostRegressor','BaggingRegressor', 'ExtraTreesRegressor',\n",
    "               'GradientBoostingRegressor', 'RandomForestRegressor']\n",
    "models = [AdaBoostRegressor(),BaggingRegressor(), ExtraTreesRegressor(),\n",
    "               GradientBoostingRegressor(), RandomForestRegressor()]\n",
    "\n",
    "mse = []\n",
    "mae = []\n",
    "msle = []\n",
    "medae = []\n",
    "r2 = []\n",
    "trained_model= []\n",
    "time_train = []\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    msle.append(mean_squared_log_error(y_test, abs(model.predict(X_test))))\n",
    "    medae.append(median_absolute_error(y_test, model.predict(X_test)))\n",
    "    r2.append(model.score(X_test, y_test))\n",
    "    \n",
    "    stop = time.time()\n",
    "    tr_time = stop - start\n",
    "    time_train.append(tr_time)\n",
    "    trained_model.append(model)\n",
    "    print(\"Model Trained\")\n",
    "models = pd.DataFrame(data = {'model': model_names, 'time_train': time_train,\n",
    "                      'MAE': mae, 'MSE': mse, 'MSLE': msle, 'MEDAE': medae, 'R2': r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:50:56.783601Z",
     "start_time": "2019-03-09T15:50:56.752344Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models = pd.concat([total_models, models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:51:03.201825Z",
     "start_time": "2019-03-09T15:51:03.170576Z"
    }
   },
   "outputs": [],
   "source": [
    "models.sort_values('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:51:22.544107Z",
     "start_time": "2019-03-09T15:51:22.528483Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVR, NuSVR, SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:56:21.731267Z",
     "start_time": "2019-03-09T15:51:41.217883Z"
    }
   },
   "outputs": [],
   "source": [
    "model_names = ['LinearSVR', 'NuSVR-Linear','NuSVR-Poly','NuSVR-RBF','NuSVR-Sigmoid',\n",
    "               'SVR-Linear','SVR-Poly','SVR-RBF','SVR-Sigmoid']\n",
    "models = [LinearSVR(), NuSVR(kernel='linear'), NuSVR(kernel='poly'),\n",
    "          NuSVR(kernel='rbf'), NuSVR(kernel = 'sigmoid'), SVR(kernel='linear'),\n",
    "          SVR(kernel='poly'), SVR(kernel='rbf'), SVR(kernel = 'sigmoid')]\n",
    "\n",
    "mse = []\n",
    "mae = []\n",
    "msle = []\n",
    "medae = []\n",
    "r2 = []\n",
    "trained_model= []\n",
    "time_train = []\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    msle.append(mean_squared_log_error(y_test, abs(model.predict(X_test))))\n",
    "    medae.append(median_absolute_error(y_test, model.predict(X_test)))\n",
    "    r2.append(model.score(X_test, y_test))\n",
    "    \n",
    "    stop = time.time()\n",
    "    tr_time = stop - start\n",
    "    time_train.append(tr_time)\n",
    "    trained_model.append(model)\n",
    "    print(\"Model Trained\")\n",
    "models = pd.DataFrame(data = {'model': model_names, 'time_train': time_train,\n",
    "                      'MAE': mae, 'MSE': mse, 'MSLE': msle, 'MEDAE': medae, 'R2':r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:56:48.028796Z",
     "start_time": "2019-03-09T15:56:48.013144Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models = pd.concat([total_models, models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:56:56.525339Z",
     "start_time": "2019-03-09T15:56:56.478789Z"
    }
   },
   "outputs": [],
   "source": [
    "models.sort_values('MSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:57:18.505757Z",
     "start_time": "2019-03-09T15:57:18.490182Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:57:28.837627Z",
     "start_time": "2019-03-09T15:57:28.420809Z"
    }
   },
   "outputs": [],
   "source": [
    "model_names = ['DecisionTreeRegressor', 'ExtraTreeRegressor']\n",
    "models = [DecisionTreeRegressor(), ExtraTreeRegressor()]\n",
    "\n",
    "mse = []\n",
    "mae = []\n",
    "msle = []\n",
    "medae = []\n",
    "r2 = []\n",
    "trained_model= []\n",
    "time_train = []\n",
    "\n",
    "for model in models:\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    mse.append(mean_squared_error(y_test, model.predict(X_test)))\n",
    "    mae.append(mean_absolute_error(y_test, model.predict(X_test)))\n",
    "    msle.append(mean_squared_log_error(y_test, abs(model.predict(X_test))))\n",
    "    medae.append(median_absolute_error(y_test, model.predict(X_test)))\n",
    "    r2.append(model.score(X_test, y_test))\n",
    "    \n",
    "    stop = time.time()\n",
    "    tr_time = stop - start\n",
    "    time_train.append(tr_time)\n",
    "    trained_model.append(model)\n",
    "    #print(model)\n",
    "models = pd.DataFrame(data = {'model': model_names, 'time_train': time_train,\n",
    "                      'MAE': mae, 'MSE': mse, 'MSLE': msle, 'MEDAE': medae, 'R2': r2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:57:37.735101Z",
     "start_time": "2019-03-09T15:57:37.719496Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models = pd.concat([total_models, models])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T15:57:50.593466Z",
     "start_time": "2019-03-09T15:57:50.546847Z"
    }
   },
   "outputs": [],
   "source": [
    "total_models.sort_values('R2', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEst Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:13:06.785985Z",
     "start_time": "2019-03-09T16:13:06.770338Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import ExtraTreesRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:16:58.887237Z",
     "start_time": "2019-03-09T16:16:58.871609Z"
    }
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"n_estimators\" : [50, 100, 200],\n",
    "    \"max_depth\" : [10, 20, 30],\n",
    "    \"max_features\" : ['sqrt', 'log2'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:17:56.400628Z",
     "start_time": "2019-03-09T16:17:56.385004Z"
    }
   },
   "outputs": [],
   "source": [
    "grid = GridSearchCV(ExtraTreesRegressor(), param_grid=param_grid, cv=3, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:18:51.015512Z",
     "start_time": "2019-03-09T16:18:05.934430Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:20:07.726630Z",
     "start_time": "2019-03-09T16:20:07.704452Z"
    }
   },
   "outputs": [],
   "source": [
    "grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:20:22.132441Z",
     "start_time": "2019-03-09T16:20:22.116834Z"
    }
   },
   "outputs": [],
   "source": [
    "model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:20:45.886762Z",
     "start_time": "2019-03-09T16:20:45.802125Z"
    }
   },
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:21:13.937984Z",
     "start_time": "2019-03-09T16:21:13.844175Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-09T16:21:14.624586Z",
     "start_time": "2019-03-09T16:21:14.593331Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
